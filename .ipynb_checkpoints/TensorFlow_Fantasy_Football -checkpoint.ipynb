{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly there are a few helper methods supplied to get data from the current fantasy football website. We will use this in the future during inference with our model created with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_players():\n",
    "    url = 'https://fantasy.premierleague.com/drf/elements/'\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def get_teams():\n",
    "    url = 'https://fantasy.premierleague.com/drf/teams/'\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def get_goalkeepers():\n",
    "    players = get_players()\n",
    "    goalkeepers = []\n",
    "    for player in players:\n",
    "        if player['element_type'] is 1:\n",
    "            goalkeepers.append(player)\n",
    "    return goalkeepers\n",
    "\n",
    "\n",
    "def get_defenders():\n",
    "    players = get_players()\n",
    "    defenders = []\n",
    "    for player in players:\n",
    "        if player['element_type'] is 2:\n",
    "            defenders.append(player)\n",
    "    return defenders\n",
    "\n",
    "\n",
    "def get_midfielders():\n",
    "    players = get_players()\n",
    "    midfielders = []\n",
    "    for player in players:\n",
    "        if player['element_type'] is 3:\n",
    "            midfielders.append(player)\n",
    "    return midfielders\n",
    "\n",
    "\n",
    "def get_forwards():\n",
    "    players = get_players()\n",
    "    forwards = []\n",
    "    for player in players:\n",
    "        if player['element_type'] is 4:\n",
    "            forwards.append(player)\n",
    "    return forwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next box, we can load up some data from a previous season (2014/2015) and use it for supervised learning. Note that by using this data with the notion of using the created model on today's and the futures data we implicitly assume that similarities in a single previous season could reoccur in a successive one. This of course could be complete bollocks. \n",
    "\n",
    "Data obtained from:\n",
    "https://www.reddit.com/r/FantasyPL/comments/38197f/i_have_complete_fantasy_premier_league_201415/\n",
    "\n",
    "Rough translation from current api keys (from json) from official website to the old data found at the above Reddit link. Not all of it is clear in terms of the translation but there is some data we can use!\n",
    "```\n",
    "Mins           = minutes\n",
    "Goals          = goals_scored\n",
    "Assists        = assists\n",
    "Clean_Sheets   = clean_sheets\n",
    "Goals_Conceded = goals_conceded\n",
    "Yellow_Cards   = yellow_cards\n",
    "Saves          = saves \n",
    "Bonus          = bonus\n",
    "PPI            = ?\n",
    "BPS            = bps\n",
    "Net_Transfers  = ? transfers_in? transfers_out? transfers_in_event? \n",
    "Value          = ? value_season? \n",
    "Points         = ? event_points? \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ID\n",
      "1 Name\n",
      "2 Team\n",
      "3 Position\n",
      "4 Availability\n",
      "5 Selection\n",
      "6 EA_Index\n",
      "7 Price\n",
      "8 ID\n",
      "9 Week\n",
      "10 Opponent\n",
      "11 Venue\n",
      "12 Mins\n",
      "13 Goals\n",
      "14 Assists\n",
      "15 Clean_Sheets\n",
      "16 Goals_Conceded\n",
      "17 Yellow_Cards\n",
      "18 Red_Cards\n",
      "19 Saves\n",
      "20 Bonus\n",
      "21 PPI\n",
      "22 BPS\n",
      "23 Net_Transfers\n",
      "24 Value\n",
      "25 Points\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "    \n",
    "    \n",
    "def get_concatenated_data():\n",
    "    \"\"\"\n",
    "    Concatenate the data found at the supplied link above\n",
    "    to something a little more convenient.\n",
    "    \"\"\"\n",
    "    def get_file_data(file_path):\n",
    "        rows = []\n",
    "        with open(file_path, encoding='ISO-8859-1') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                rows.append(row)\n",
    "        return rows\n",
    "    \n",
    "    details = get_file_data('Player_Details.csv')\n",
    "    data = get_file_data('Player_Data.csv')\n",
    "    concatenated_rows = []\n",
    "    \n",
    "    for i, data_row in enumerate(data):\n",
    "        if i != 0:\n",
    "            player_id = data_row[0]\n",
    "            for j, player_row in enumerate(details):\n",
    "                if j != 0:\n",
    "                    if player_row[0] == player_id:\n",
    "                        temp_row = []\n",
    "                        temp_row.extend(player_row)\n",
    "                        temp_row.extend(data_row)\n",
    "                        concatenated_rows.append(temp_row)\n",
    "    return concatenated_rows, details[0] + data[0]\n",
    "           \n",
    "    \n",
    "all_old_data, help_header = get_concatenated_data()\n",
    "\n",
    "for i, value in enumerate(help_header):\n",
    "    print(i, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11033 available training examples.\n",
      "2759 available validation examples.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "desired_indices = [6, 7, 12, 13, 14, 16, 17, 18, 20, 21, 22, 24, 25]\n",
    "desired_player_position = \"Midfielder\"\n",
    "other_desired_player_position = \"Forward\"\n",
    "normalise_data = False\n",
    "\n",
    "\n",
    "def get_desired_data(data, desired_indices, player_position, other_player_position):\n",
    "    \"\"\"\n",
    "    Using the above features create supervised data for\n",
    "    TensorFlow neural networks.\n",
    "    \"\"\"\n",
    "    previous_id = int(data[0][0])\n",
    "    previous_week = int(data[0][9])\n",
    "    previous_value = float(data[0][25])\n",
    "    total_data_size = len(data)\n",
    "\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "    index = 1\n",
    "    while index < total_data_size:\n",
    "        current_id = int(data[index][0])\n",
    "        current_week = int(data[index][9])\n",
    "        position = data[index][3]\n",
    "\n",
    "        correct_position = position == player_position or position == other_player_position\n",
    "        correct_id = previous_id == current_id\n",
    "        corrent_week = (current_week - 1) == previous_week\n",
    "\n",
    "        if (correct_position and correct_id and corrent_week):\n",
    "            current_value = float(data[index][25])    \n",
    "            features.append([float(data[index - 1][i]) for i in desired_indices])\n",
    "            labels.append(current_value)\n",
    "            previous_value = current_value\n",
    "        index += 1\n",
    "\n",
    "        previous_id = current_id\n",
    "        previous_week = current_week\n",
    "    return np.array(features), np.array(labels).reshape((-1, 1))\n",
    "\n",
    "features, labels = get_desired_data(all_old_data,\n",
    "                                    desired_indices,\n",
    "                                    desired_player_position,\n",
    "                                    other_desired_player_position)\n",
    "if normalise_data:\n",
    "    features = normalize(features)\n",
    "    labels = normalize(labels)\n",
    "\n",
    "train_split = int(0.8 * len(features))\n",
    "train_x = features[0:train_split]\n",
    "train_y = labels[0:train_split]\n",
    "valid_x = features[:-train_split]\n",
    "valid_y = labels[:-train_split]\n",
    "\n",
    "print(len(train_x), \"available training examples.\")\n",
    "print(len(valid_x), \"available validation examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 17299  | total loss: \u001b[1m\u001b[32m4.48019\u001b[0m\u001b[0m | time: 0.789s\n",
      "| Adam | epoch: 100 | loss: 4.48019 - binary_acc: 0.2430 -- iter: 11008/11033\n",
      "Training Step: 17300  | total loss: \u001b[1m\u001b[32m4.15429\u001b[0m\u001b[0m | time: 1.798s\n",
      "| Adam | epoch: 100 | loss: 4.15429 - binary_acc: 0.2468 | val_loss: 3.76403 - val_acc: 0.1182 -- iter: 11033/11033\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, train_x.shape[1]])\n",
    "net = tflearn.fully_connected(net, 64, activation='relu',\n",
    "                              regularizer='L2', weight_decay=0.001)\n",
    "net = tflearn.dropout(net, 0.8)\n",
    "net = tflearn.fully_connected(net, 64, activation='relu',\n",
    "                              regularizer='L2', weight_decay=0.001)\n",
    "net = tflearn.dropout(net, 0.8)\n",
    "net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "\n",
    "net = tflearn.regression(net, optimizer='adam', loss='mean_square')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(train_x, train_y, n_epoch=100, validation_set=(valid_x, valid_y), \n",
    "          shuffle=True, show_metric=True, run_id=\"dense_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2759\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npwwASeqUASBJnTIAJKlT/w/AdUcg39kNtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15ec8fa668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictions = model.predict(valid_x)\n",
    "\n",
    "differences = []\n",
    "for i, prediction in enumerate(predictions):\n",
    "    differences.append(abs(prediction - valid_y[i].tolist()))\n",
    "differences = np.array(differences).reshape(-1).tolist()\n",
    "\n",
    "print(len(differences))\n",
    "_ = plt.hist(differences, bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
